# AutoEncoders

## Denoising Autoencoder

Autoencoders are Neural Networks which are commonly used for feature selection and extraction. 
However, when there are more nodes in the hidden layer than there are inputs, the Network is risking to learn the so-called “Identity Function”, 
also called “Null Function”, meaning that the output equals the input, marking the Autoencoder useless.

Denoising Autoencoders solve this problem by corrupting the data on purpose by adding some noise to inputs. 
then the network is expected to recover the original input as it's output. That way, the risk of learning the identity function instead of extracting features is eliminated.

Here I use pytorch to implement a simple Denoising Autoencoder. I use the MNIST dataset and add gaussian random noise. 
below you can see an example from the network after 50 epochs

![DAE example result](https://github.com/arssly/AutoEncoders/raw/master/dae.png)


## Conditional Variational Autoencoder

The variational autoencoder or VAE is a directed graphical generative model which has obtained excellent results and is among the state of the art approaches to generative modeling. It assumes that the data is generated by some random process, involving an unobserved continuous random variable z. it is assumed that the z is generated from some prior distribution P_θ(z) and the data is generated from some condition distribution P_θ(X|Z), where X represents that data. The z is sometimes called the hidden representation of data X.

The one problem for generating data with VAE is we do not have any control over what kind of data it generates. For example, if we train a VAE with the MNIST data set and try to generate images by feeding Z ~ N(0,1) into the decoder, it will also produce different random digits. If we train it well, the images will be good but we will have no control over what digit it will produce. For example, you can not tell the VAE to produce an image of digit ‘2’.
For this, we need to have a little change to our VAE architecture. Let’s say, given an input Y(label of the image) we want our generative model to produce output X(image). So, the process of VAE will be modified as the following: given observation y, z is drawn from the prior distribution P_θ(z|y), and the output x is generated from the distribution P_θ(x|y, z). Please note that, for simple VAE, the prior is P_θ(z) and the output is generated by P_θ(x|z).
So, here the encoder part tries to learn q_φ(z|x,y), which is equivalent to learning hidden representation of data X or encoding the X into the hidden representation conditioned y. The decoder part tries to learn P_θ(X|z,y) which decoding the hidden representation to input space conditioned by y.

Here I use pytorch to implement a simple Conditional Variational Autoencoder on MNIST dataset. 

![generated result by trained CVAE for every digit](https://github.com/arssly/AutoEncoders/raw/master/cvae.png)
